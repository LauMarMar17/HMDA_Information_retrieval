{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A vector space model implementation using NLTK (Natural Language ToolKit) and Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the functions provided by NLTK to perform tokenizing considering punctuation signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import required functions to filter-out stopwords for the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the function that implements the Porter's stemming algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we provide a sample document collection, containing 9 (very brief) documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is aimed at preprocessing each document in the collection. We write a function that receives a textual document in string format, and returns a list containing all STEMS in the collection whose associated token is longer than 2 characters and is NOT an (English) stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(doc):\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = wordpunct_tokenize(doc)\n",
    "    clean = [token.lower() for token in tokens if token.lower() not in stopset and len(token) > 2]\n",
    "    final = [stemmer.stem(word) for word in clean]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now, for instance, tokenize the first document in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['survey', 'user', 'opinion', 'comput', 'system', 'respons', 'time']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_document(sample_corpus[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all documents in the collection have been preprocessed, we need to create a dictionary containing the mappings WORD_ID -> WORD. This dictionary is required to create the vector-based word representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "def create_dictionary(docs):\n",
    "    pdocs = [preprocess_document(doc) for doc in docs]\n",
    "    dictionary = corpora.Dictionary(pdocs)\n",
    "    dictionary.save('/tmp/vsm.dict')\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us call the create_dictionary function feeding it with the complete corpus. Note that it is possible to save the generated dictionary to disk if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(34 unique tokens: ['abc', 'applic', 'comput', 'human', 'interfac']...)\n"
     ]
    }
   ],
   "source": [
    "dict = create_dictionary(sample_corpus)\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have built the dictionary containing the vocabulary that we will use for indexing. Now we write a function that create the bag of words-based representation for each document in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2bows(corpus, dictionary):\n",
    "    docs = [preprocess_document(d) for d in corpus]\n",
    "    vectors = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    corpora.MmCorpus.serialize('/tmp/vsm_docs.mm', vectors)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is possible to save the generated BOW-based corpus if we wish. For doing so, we need to import the corpora module from Gensim. Let us now generate the BOWs for the complete corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(2, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(4, 1), (10, 1), (12, 1), (13, 1), (14, 1)], [(3, 1), (10, 2), (13, 1), (15, 1), (16, 1)], [(8, 1), (11, 1), (12, 1), (17, 1), (18, 1), (19, 1), (20, 1)], [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1)], [(24, 1), (26, 1), (27, 1), (28, 1)], [(24, 1), (26, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1)], [(9, 1), (26, 1), (29, 1)]]\n"
     ]
    }
   ],
   "source": [
    "bows = docs2bows(sample_corpus, dict)\n",
    "print(bows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are pairs (word identifier, frequency). Let us now convert them into something a bit more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('abc', 1), ('applic', 1), ('comput', 1), ('human', 1), ('interfac', 1), ('lab', 1), ('machin', 1)]\n",
      "[('comput', 1), ('opinion', 1), ('respons', 1), ('survey', 1), ('system', 1), ('time', 1), ('user', 1)]\n",
      "[('interfac', 1), ('system', 1), ('user', 1), ('ep', 1), ('manag', 1)]\n",
      "[('human', 1), ('system', 2), ('ep', 1), ('engin', 1), ('test', 1)]\n",
      "[('respons', 1), ('time', 1), ('user', 1), ('error', 1), ('measur', 1), ('perceiv', 1), ('relat', 1)]\n",
      "[('binari', 1), ('gener', 1), ('random', 1), ('tree', 1), ('unord', 1)]\n",
      "[('tree', 1), ('graph', 1), ('intersect', 1), ('path', 1)]\n",
      "[('tree', 1), ('graph', 1), ('minor', 1), ('order', 1), ('quasi', 1), ('well', 1), ('width', 1)]\n",
      "[('survey', 1), ('graph', 1), ('minor', 1)]\n"
     ]
    }
   ],
   "source": [
    "for v in bows:\n",
    "    tvec = [(dict[id], freq) for (id, freq) in v]\n",
    "    print(tvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are basically TF-weighted vectors. We now want to convert these vectors into their TF-IDF weighted counterparts. We need, however, to import the models module from Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "def create_TF_IDF_model(corpus):\n",
    "    dictionary = create_dictionary(corpus)\n",
    "    docs2bows(corpus, dictionary)\n",
    "    loaded_corpus = corpora.MmCorpus('/tmp/vsm_docs.mm')\n",
    "    tfidf = models.TfidfModel(loaded_corpus)\n",
    "    return tfidf, dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create the TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<gensim.models.tfidfmodel.TfidfModel object at 0x7f38f0681b38>, <gensim.corpora.dictionary.Dictionary object at 0x7f38f0674e10>)\n"
     ]
    }
   ],
   "source": [
    "tfidfm = create_TF_IDF_model(sample_corpus)\n",
    "print(tfidfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, a complex object is returned that contains the TF-IDF model and the associated dictionary. Let us now take a closer look of such a TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id2word': None, 'wlocal': <function identity at 0x7f38f1b77048>, 'wglobal': <function df2idf at 0x7f38f1446840>, 'normalize': True, 'num_docs': 9, 'num_nnz': 50, 'idfs': {0: 3.1699250014423126, 1: 3.1699250014423126, 2: 2.1699250014423126, 3: 2.1699250014423126, 4: 2.1699250014423126, 5: 3.1699250014423126, 6: 3.1699250014423126, 7: 3.1699250014423126, 8: 2.1699250014423126, 9: 2.1699250014423126, 10: 1.5849625007211563, 11: 2.1699250014423126, 12: 1.5849625007211563, 13: 2.1699250014423126, 14: 3.1699250014423126, 15: 3.1699250014423126, 16: 3.1699250014423126, 17: 3.1699250014423126, 18: 3.1699250014423126, 19: 3.1699250014423126, 20: 3.1699250014423126, 21: 3.1699250014423126, 22: 3.1699250014423126, 23: 3.1699250014423126, 24: 1.5849625007211563, 25: 3.1699250014423126, 26: 1.5849625007211563, 27: 3.1699250014423126, 28: 3.1699250014423126, 29: 2.1699250014423126, 30: 3.1699250014423126, 31: 3.1699250014423126, 32: 3.1699250014423126, 33: 3.1699250014423126}, 'smartirs': None, 'slope': 0.25, 'pivot': None, 'eps': 1e-12, 'cfs': None, 'dfs': {0: 1, 1: 1, 2: 2, 3: 2, 4: 2, 5: 1, 6: 1, 7: 1, 8: 2, 9: 2, 10: 3, 11: 2, 12: 3, 13: 2, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 3, 25: 1, 26: 3, 27: 1, 28: 1, 29: 2, 30: 1, 31: 1, 32: 1, 33: 1}, 'term_lengths': None}\n"
     ]
    }
   ],
   "source": [
    "print(tfidfm[0].__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally create a function that given the corpus and an user-provided query provides a document ranking sorted in descending order of relevance (according to the cosine measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from gensim import similarities\n",
    "\n",
    "def launch_query(corpus, q, filename='/tmp/vsm_docs.mm'):\n",
    "    tfidf, dictionary = create_TF_IDF_model(corpus)\n",
    "    loaded_corpus = corpora.MmCorpus(filename)\n",
    "    index = similarities.MatrixSimilarity(loaded_corpus, num_features=len(dictionary))\n",
    "    pq = preprocess_document(q)\n",
    "    vq = dictionary.doc2bow(pq)\n",
    "    qtfidf = tfidf[vq]\n",
    "    sim = index[qtfidf]\n",
    "    ranking = sorted(enumerate(sim), key=itemgetter(1), reverse=True)\n",
    "    for doc, score in ranking:\n",
    "        print(\"[ Score = \" + \"%.3f\" % round(score,3) + \" ] \" + corpus[doc]); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can launch any query we see fit to our newly created Information Retrieval engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Score = 0.547 ] System and human system engineering testing of EPS\n",
      "[ Score = 0.486 ] The EPS user interface management system\n",
      "[ Score = 0.475 ] Human machine interface for lab abc computer applications\n",
      "[ Score = 0.173 ] A survey of user opinion of computer system response time\n",
      "[ Score = 0.000 ] Relation of user perceived response time to error measurement\n",
      "[ Score = 0.000 ] The generation of random binary unordered trees\n",
      "[ Score = 0.000 ] The intersection graph of paths in trees\n",
      "[ Score = 0.000 ] Graph minors IV Widths of trees and well quasi ordering\n",
      "[ Score = 0.000 ] Graph minors A survey\n"
     ]
    }
   ],
   "source": [
    "launch_query(sample_corpus, \"human interface systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
