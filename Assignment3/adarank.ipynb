{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LICENSE\n",
    "\n",
    "This notebook uses the Ruey-Cheng Chen library to train and test an AdaRank model:\n",
    "https://github.com/rueycheng/AdaRank\n",
    "\n",
    "The model is trained with a training svmlight file and evaluated with a test svmlight file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "\n",
    "# adarank_lib contains the AdaRank implementation form Ruey-Cheng Chen library\n",
    "from adarank_lib.adarank import AdaRank\n",
    "from adarank_lib.metrics import NDCGScorer, APScorer, PScorer\n",
    "from adarank_lib.utils import load_docno, print_ranking\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Load excels labeled and turn them into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel Files\n",
    "\n",
    "excel_doc = \"../data/Loinc/loinc_dataset_labels-v2.xlsx\"\n",
    "extended_excel_doc = \"../data/Loinc/extended_loinc_dataset-v2.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loing_dataset_labels-v2.xlsx columns:\n",
    "Original columns\n",
    "* __Loinc_num__: loinc identifier number.\n",
    "* __long_common_name__: description\n",
    "* __component__: name of the component\n",
    "* __system__: ¿?\n",
    "* __property__: ¿?\n",
    "\n",
    "Added columns\n",
    "* __doc_numb__: document number for each query. #1 to #67 for each query (201 in total)\n",
    "* __qid__: Query id. [1, 2, 3 ]. For the extended file [1,2 3, 4, 5, 6]\n",
    "* __Label__: Relevance number 0: irrelevant, 1: relevant, 2: super-relevant.\n",
    "  \n",
    "_(*) Nota: Explicar en que nos hemos basado para poner la etiqueta de relevancia. P.e. para el query \"Glucose in Blood\", buscamos todas las entradas donde esté la palabra \"glucose\" y le añadimos un 1. Luego buscamos la palabra \"blood\" y cuando coincida con alguna de las entradas anteriores le sumamos 1. Tendremos relevancia 1 cuando aparezca la palabra \"glucose\" y relevancia 2 cuando aparezcan \"glucose\"+\"blood\"._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframes\n",
    "\n",
    "# helper.excel_to_df: Reads an excel file and returns a dataframe with all the sheets concatenated\n",
    "df = helper.excel_to_df(excel_doc)\n",
    "extended_df = helper.excel_to_df(extended_excel_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create the Train and Test sets based on Features (X), Labels (y), and Queries (qid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: helper.df_to_svmlight_flies()\n",
    "Transforms a dataframe into svmlight files\n",
    "  * Takes the column \"long_common_name\" and creates anotherone called \"cleaned_text\" with the text preprocesed:\n",
    "    - Remove punctuation\n",
    "    - Tokenize\n",
    "    - Remove small words\n",
    "    - Remove stopwords\n",
    "    - Stemize ¿?\n",
    "    - Lemmatize ¿?\n",
    "    - Join sentences\n",
    "  * Once the text is cleaned uses _TfidfVectorizer()_ to get the document's features.\n",
    "  * Creates a new column \"features\" for each document with these features.\n",
    "  * Split the whole df in 70% train and 30% test:\n",
    "    - First suffles the dataset so we dont have the qid 1 docs at the begining and the qid3 at the end.\n",
    "    - Takes the first 70% of the shuffled df \n",
    "    - Takes the last 30% of the shuffled df as test and sort it by qid\n",
    "  * Takes the column \"features\" from Train and Test as X_train and X_test respectively. Takes \"label\" as y_train and y_test. Takes \"qid\" as qid_train, qid_test.\n",
    "  * Uses sklearn.dataset function \"_dump_svmlight_file_\" to dump the dataset in svmlight / libsvm file format. This format is a text-based format, with one sample per line. It does not store zero valued features hence is suitable for sparse dataset. Finally creates .dat files with all this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\laura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\laura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Svmlight files\n",
    "os.chdir('../data/Loinc/svmlight_files')\n",
    "file, train_file, test_file  = helper.df_to_svmlight_files(df)\n",
    "extended_file, extended_train_file, extended_test_file = helper.df_to_svmlight_files(extended_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adarank_score_and_ranking(excel_doc, sc=\"NDCGScorer\"):\n",
    "    \"\"\" \n",
    "    Obtain a Score and Ranking for the AdaRank algorithm:\n",
    "    1. NDCGScorer (Normalized Discounted Cumulative Gain scorer) Default:\n",
    "        A measure of ranking quality that is often used to measure effectiveness \n",
    "        of web search engine algorithms or related applications.\n",
    "    2. APScorer (Average Precision scorer):\n",
    "        ...\n",
    "    3. PScorer (Precision scorer):\n",
    "        ...\n",
    "    \"\"\"\n",
    "    if excel_doc == extended_excel_doc:\n",
    "        tr_file = extended_train_file\n",
    "        tst_file = extended_test_file\n",
    "        if sc == \"NDCGScorer\":\n",
    "            ranking_file = 'extended_NDCG_ranking.txt'\n",
    "        elif sc == \"APScorer\":\n",
    "            ranking_file = 'extended_AP_ranking.txt'\n",
    "        elif sc == \"PScorer\":\n",
    "            ranking_file = 'extended_P_ranking.txt'\n",
    "    else:\n",
    "        tr_file = train_file\n",
    "        tst_file = test_file\n",
    "        if sc == \"NDCGScorer\":\n",
    "            ranking_file = 'NDCG_ranking.txt'\n",
    "        elif sc == \"APScorer\":\n",
    "            ranking_file = 'AP_ranking.txt'\n",
    "        elif sc == \"PScorer\":\n",
    "            ranking_file = 'P_ranking.txt'\n",
    "   \n",
    "    X_train, y_train, qid_train = load_svmlight_file(tr_file, query_id=True)\n",
    "    X_test, y_test, qid_test = load_svmlight_file(tst_file, query_id=True)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Run AdaRank for 100 iterations optimizing for NDCG@10 / AP@10 / P@10. \n",
    "    When no improvement is made within the previous 10 iterations, \n",
    "    the algorithm will stop.\n",
    "    '''\n",
    "    if sc == \"NDCGScorer\":\n",
    "        model = AdaRank(max_iter=100, stop=10, scorer=NDCGScorer(k=10)).fit(X_train, y_train, qid_train)\n",
    "    elif sc == \"APScorer\":\n",
    "        model = AdaRank(max_iter=100, stop=10, scorer=APScorer()).fit(X_train, y_train, qid_train)\n",
    "    elif sc == \"PScorer\":\n",
    "        model = AdaRank(max_iter=100, stop=10, scorer=PScorer()).fit(X_train, y_train, qid_train)\n",
    "    pred = model.predict(X_test, qid_test)\n",
    "    \n",
    "    # nDCG scores\n",
    "    if sc == \"NDCGScorer\":\n",
    "        for k in (1, 2, 3, 4, 5, 10, 20):\n",
    "                score = NDCGScorer(k=k)(y_test, pred, qid_test).mean()\n",
    "                print('nDCG@{}\\t{}'.format(k, score))\n",
    "    # AP scores\n",
    "    elif sc == \"APScorer\":\n",
    "        score = APScorer()(y_test, pred, qid_test).mean()\n",
    "        print('AP\\t{}'.format(score))\n",
    "    # Precision scores\n",
    "    elif sc == \"PScorer\":\n",
    "        score = PScorer()(y_test, pred, qid_test).mean()\n",
    "        print('P\\t{}'.format(score))\n",
    "        \n",
    "    \n",
    "    # Return ranking\n",
    "    docno = load_docno(tst_file, letor=False)\n",
    "    os.chdir('../../../Assignment3/Rankings')\n",
    "    print_ranking(qid_test, docno, pred, output=open(ranking_file, 'w'))\n",
    "    os.chdir(\"../../data/Loinc/svmlight_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG scores for original dataset:\n",
      "nDCG@1\t0.0\n",
      "nDCG@2\t0.1289509357448472\n",
      "nDCG@3\t0.1289509357448472\n",
      "nDCG@4\t0.2169736432690442\n",
      "nDCG@5\t0.2481896027351573\n",
      "nDCG@10\t0.34795917795457837\n",
      "nDCG@20\t0.34795917795457837\n",
      "\n",
      "nDCG scores for extended dataset:\n",
      "nDCG@1\t0.0\n",
      "nDCG@2\t0.0\n",
      "nDCG@3\t0.05109559939712153\n",
      "nDCG@4\t0.16688637950478555\n",
      "nDCG@5\t0.16688637950478555\n",
      "nDCG@10\t0.19912411344099734\n",
      "nDCG@20\t0.2756690561705189\n"
     ]
    }
   ],
   "source": [
    "# Get scores for excel documents\n",
    "print('nDCG scores for original dataset:')\n",
    "get_adarank_score_and_ranking(excel_doc)\n",
    "\n",
    "\n",
    "print('\\nnDCG scores for extended dataset:')\n",
    "get_adarank_score_and_ranking(extended_excel_doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP scores for original dataset:\n",
      "AP\t0.25767195767195766\n",
      "\n",
      "AP scores for extended dataset:\n",
      "AP\t0.16508295561362615\n"
     ]
    }
   ],
   "source": [
    "print('AP scores for original dataset:')\n",
    "get_adarank_score_and_ranking(excel_doc, \"APScorer\")\n",
    "\n",
    "\n",
    "print('\\nAP scores for extended dataset:')\n",
    "get_adarank_score_and_ranking(extended_excel_doc, \"APScorer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision scores for original dataset:\n",
      "P\t0.08293460925039872\n",
      "\n",
      "Precision scores for extended dataset:\n",
      "P\t0.06430557275967905\n"
     ]
    }
   ],
   "source": [
    "print('Precision scores for original dataset:')\n",
    "get_adarank_score_and_ranking(excel_doc, \"PScorer\")\n",
    "\n",
    "\n",
    "print('\\nPrecision scores for extended dataset:')\n",
    "get_adarank_score_and_ranking(extended_excel_doc, \"PScorer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info_retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
